from __future__ import division
import os
import time
import math
from glob import glob
import tensorflow as tf
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from six.moves import xrange

from ops import *
from utils import *

class DCGAN(object):
    def __init__(self, sess, input_height=108, input_width=108,
                 batch_size=100, sample_num=100, output_height=64, output_width=64,
                 z_dim=100, c_dim=1, dataset_name='default',
                 checkpoint_dir=None, sample_dir=None, data_dir='./data'):
        """
        Args:
          sess: TensorFlow session
          input_size: The size of input image.
          batch_size: The size of batch. Should be specified before training.
          z_dim: Dimension of dim for Z. [100]
          c_dim: Dimension of image color. For grayscale input, set to 1. [1]
        """
        
        self.sess = sess

        self.batch_size = batch_size
        self.sample_num = sample_num

        self.input_height = input_height
        self.input_width = input_width
        self.output_height = output_height
        self.output_width = output_width

        self.z_dim = z_dim
        self.c_dim = c_dim

        self.dataset_name = dataset_name
        self.checkpoint_dir = checkpoint_dir

        self.build_model()

    def discriminator(self, image, reuse=False, train=True):
        with tf.variable_scope("discriminator", reuse=reuse):
            #######################################################
            # TODO: Define discrminator network structure here. op.py
            # includes some basic layer functions for you to use.
            # Please use batch normalization layer after conv layer.
            # And use 'train' argument to indicate the mode of bn.
            #######################################################
            h1 = conv2d(image, 32, name='d_cv1')
            h1r = lrelu(batch_norm(h1, train=train, name="d_bn1"))
            
            #h2 = conv2d(h1r, 128, name='d_cv2')
            #h2r = lrelu(batch_norm(h2, train=train, name="d_bn2"))
            
            #h3 = conv2d(image, 300, name='d_cv3')
            #h3r = lrelu(batch_norm(h3, train=train, name="d_bn3"))
            
            h4r = lrelu(linear(tf.reshape(h1r, [self.batch_size, -1]), 100, scope='d_fc4'))

            h5 = linear(h4r, 1, scope="d_fc5")
            
            return tf.nn.sigmoid(h5)

        
    def generator(self, z, reuse=False, train=True):
        with tf.variable_scope("generator", reuse=reuse):
            #######################################################
            # TODO: Define decoder network structure here. The size
            # of output should match the size of images. Image scale
            # in DCGAN is [-1, +1], so you need to add a tanh layer
            # before the output. Also use batch normalization layer
            # after deconv layer, and use 'train' argument to indicate
            # the mode of bn layer. Note that when sampling images
            # using trained model, you need to set train='False'.
            #######################################################
            # h1r = lrelu(linear(z, 200, 'g_fc1'))
            
            h2 = linear(z, 7*7*self.c_dim*8, 'g_fc2')
            h2f = tf.reshape(h2, [-1, 7, 7, self.c_dim*8])
            h2r = lrelu(batch_norm(h2f, train=train, name="g_bn2"))
            
            h3 = deconv2d(h2r, [self.batch_size, 14, 14, self.c_dim*32], name='g_cv3')
            h3r = lrelu(batch_norm(h3, train=train, name="g_bn3"))
            
            h4 = deconv2d(h3r, [self.batch_size, 28, 28, self.c_dim*16], name='g_cv4')
            h4r = lrelu(batch_norm(h4, train=train, name="g_bn4"))

            h5 = deconv2d(h4r, [self.batch_size, 28, 28, self.c_dim], d_h=1, d_w=1, name='g_cv5')
            
            return tf.nn.tanh(h5), h5

    def build_model(self):
        #######################################################
        # TODO: In this build_model function, define inputs,
        # operations on inputs and loss of DCGAN. For input,
        # you need to define it as placeholders. Discriminator
        # loss has two parts: cross entropy for real images and
        # cross entropy for fake images generated by generator.
        # Set reuse=True for discriminator when calculating the
        # second cross entropy. Define two different loss terms
        # for discriminator and generator, and save them as
        # self.d_loss and self.g_loss respectively.
        #######################################################
        image_dims = [self.output_height, self.output_width, self.c_dim]
        self.input = tf.placeholder(tf.float32, [self.batch_size]+image_dims, name="input")       
        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')        
        
        self.G      = self.generator(self.z, reuse=False, train=True)
        self.D, self.D_logits      = self.discriminator(self.input, reuse=False, train=True)
        self.sample = self.generator(self.z, reuse=True, train=False)
        self.D_f, self.D_logits_f    = self.discriminator(self.G, reuse=True)
        
        # Discriminator loss: cross entropy for real + cross entropy for fake generated by generator
        # cross entropy loss: https://blog.csdn.net/mao_xiao_feng/article/details/53382790
        self.d_loss_real = tf.reduce_mean(
                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits, labels=tf.ones_like(self.D)))
        self.d_loss_fake = tf.reduce_mean(
                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_f, labels=tf.zeros_like(self.D_f)))
        self.d_loss = self.d_loss_real + self.d_loss_fake
        
        # Generator loss
        self.g_loss = tf.reduce_mean(
                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_f, labels=tf.ones_like(self.D_f))) # output of D-net, actual label

        # define var lists for generator and discriminator
        t_vars = tf.trainable_variables()

        self.d_vars = [var for var in t_vars if 'd_' in var.name]
        self.g_vars = [var for var in t_vars if 'g_' in var.name]

        self.saver = tf.train.Saver()

    def train(self, config):
        # create two optimizers for generator and discriminator,
        # and only update the corresponding variables.
        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \
            .minimize(self.d_loss, var_list=self.d_vars)
        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \
            .minimize(self.g_loss, var_list=self.g_vars)
        try:
            self.sess.run(tf.global_variables_initializer())
        except:
            tf.initialize_all_variables().run()

        # load data
        if config.dataset == 'mnist':
            mnist = tf.contrib.learn.datasets.load_dataset("mnist")
            data = mnist.train.images
            data = data.astype(np.float32)
            data_len = data.shape[0]
            data = np.reshape(data, [-1, 28, 28, 1])
            data = data * 2.0 - 1.0
        else:
            sample_files = self.data[0:self.sample_num]
            sample = [
            get_image(sample_file,
                    input_height=self.input_height,
                    input_width=self.input_width,
                    resize_height=self.output_height,
                    resize_width=self.output_width,
                    crop=self.crop,
                    grayscale=self.grayscale) for sample_file in sample_files]
            if (self.grayscale):
                sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None]
            else:
                sample_inputs = np.array(sample).astype(np.float32)
        
        self.d = []
        self.g = []
        counter = 1
        could_load, checkpoint_counter = self.load(self.checkpoint_dir)
        if could_load:
            counter = checkpoint_counter
            print(" [*] Load SUCCESS")
        else:
            print(" [!] Load failed...")

        for epoch in xrange(config.epoch):
            batch_idxs = min(data_len, config.train_size) // config.batch_size

            for idx in xrange(0, batch_idxs):
                batch_images = data[idx * config.batch_size:(idx + 1) * config.batch_size, :]
                #######################################################
                # TODO: Train your model here. Sample hidden z from
                # standard uniform distribution. In each step, run g_optim
                # twice to make sure that d_loss does not go to zero.
                # print the loss terms at each training step to monitor
                # the training process. Print sample images every
                # config.print_step steps.You may use function
                # save_images in utils.py to save images.
                #######################################################
                sample_z = np.random.uniform(-1, 1, [self.batch_size, self.z_dim]).astype(np.float32)
                #smpl_idx = np.random.choice(data_len, self.batch_size, replace=False)
                #sample_inputs = data[smpl_idx, :]
                
                # Update D-net
                _, step_d_loss = self.sess.run([d_optim, self.d_loss], feed_dict={self.input_image: batch_images,
                                                      self.z: sample_z})
                
                # Update G-net
                _ = self.sess.run(g_optim, feed_dict={self.z: sample_z})
                
                # _ = self.sess.run(g_optim, feed_dict={self.z: sample_z})
                
                # Run G-net twice: d_loss does not go to zero
                _, step_g_loss = self.sess.run([g_optim, self.g_loss], feed_dict={self.z: sample_z})
                
                self.d.append(step_d_loss)
                self.g.append(step_g_loss)

                # print the loss terms at each training step
                if np.mod(counter, 200) == 1:
                    print("Epoch[%2d] Batch[%3d/%3d] d_loss: %.6f, g_loss: %.6f" % (epoch, idx, batch_idxs, step_d_loss, step_g_loss))
                
                # Print sample images every 200 steps
                if np.mod(counter, 100) == 1:
                    samples, smpl_d_loss, smpl_g_loss = self.sess.run([self.sample, self.d_loss, self.g_loss],
                                                feed_dict={self.z: sample_z,
                                                           self.input_image: batch_images})
                    
                    save_images(samples, [10, 10], './{}/train_{:02d}_{:02d}.png'.format(config.sample_dir, epoch, idx))
                    # imshow('./{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))
                    # print("[Sample] d_loss: %.8f, g_loss: %.8f" % (smpl_d_loss, smpl_g_loss))
          
                #######################################################
                #                   end of your code
                #######################################################

                counter += 1
                if np.mod(counter, 500) == 1:
                    self.save(config.checkpoint_dir, counter)

        plt.figure(2)
        plt.title('Training Loss')
        plt.xlabel('training step')
        plt.ylabel('loss')
        plt.plot(self.g)
        plt.plot(self.d)
    
    @property
    def model_dir(self):
        return "{}_{}_{}_{}".format(
            self.dataset_name, self.batch_size,
            self.output_size, self.output_size)

    def save(self, checkpoint_dir, step):
        model_name = "DCGAN.model"
        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)

        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        self.saver.save(self.sess,
                        os.path.join(checkpoint_dir, model_name),
                        global_step=step)

    def load(self, checkpoint_dir):
        import re
        print(" [*] Reading checkpoints...")
        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)

        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)
            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))
            counter = int(next(re.finditer("(\d+)(?!.*\d)", ckpt_name)).group(0))
            print(" [*] Success to read {}".format(ckpt_name))
            return True, counter
        else:
            print(" [*] Failed to find a checkpoint")
            return False, 0
